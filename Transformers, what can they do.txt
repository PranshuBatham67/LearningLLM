GOOGLE COLAB NOTEBOOK : https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb#scrollTo=AqrAna6rZL0J

# In this section, we will look at what Transformer models can do and use our first tool from the ðŸ¤— Transformers library: the pipeline() function.

# Transformers are everywhere!
-------------------------------

#Transformer models are used to solve all kinds of tasks across different modalities, including natural language processing (NLP), computer vision, audio processing, and more.


NOTE : The most basic object in the ðŸ¤— Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:

CODE 
----
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")

Output : [{'label': 'POSITIVE', 'score': 0.9598047137260437}]

# Important observation of the above code 
------------------------------------------
1.By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English.

2.The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.


# There are three main steps involved when you pass some text to a pipeline:

1.The text is preprocessed into a format the model can understand.
2.The preprocessed inputs are passed to the model.
3.The predictions of the model are post-processed, so you can make sense of them.


1. Preprocessing (before passing text to the model):
This step transforms your raw input text into a format that the model can understand and process effectively. Common preprocessing steps include:

    # Tokenization: Splitting the text into smaller units like words or subwords (tokens). For instance, "waiting" might be split into "wait" and "ing" depending on the tokenizer.
    
    # Adding special tokens: Many models require special markers like [CLS] (classification token) at the start or [SEP] (separator) tokens at the end.
      
    # Converting tokens to IDs: Mapping each token to a unique integer ID from the model's vocabulary.
    
    # Creating attention masks: Indicating which tokens should be attended to (useful for padding sequences to the same length).

In simple terms:
Preprocessing prepares your text so that it looks like the data the model was trained on, making it possible for the model to understand and analyze the input.


2. Post-processing (after getting model output):
Once the model makes its prediction, the raw output (often numerical scores or token IDs) needs to be transformed into human-readable or usable formats. Common post-processing steps include:
   # Decoding token IDs back into words: If the model outputs token IDs, these are converted back into readable text.
   
   # Interpreting scores: Turning raw scores or logits into labels like "POSITIVE" or "NEGATIVE" with associated confidence scores.
   
   # Filtering or thresholding: Deciding which labels to assign based on confidence scores.

In simple terms:
Post-processing takes the modelâ€™s raw output and converts it into a clear, understandable result, like a label and confidence score.

# He same general principle applies to all machine learning pipelines: data in a human-readable format â†’ preprocessing â†’ numerical data for the model â†’ model inference â†’ raw numerical output â†’ post-processing â†’ human-readable result.


Ex:

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load tokenizer and model
model_name = "distilbert-base-uncased-finetuned-sst-2-english"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Your raw input text
text = "I've been waiting for a HuggingFace course my whole life."

# 1. Preprocessing: Tokenize the input text
inputs = tokenizer(text, return_tensors="pt")  # returns input IDs and attention mask

# Now, 'inputs' contains:
# - input_ids: token IDs
# - attention_mask: mask indicating real tokens vs padding

print("Preprocessed input:", inputs)

# 2. Pass through the model
outputs = model(**inputs)

# The raw output logits
logits = outputs.logits

# 3. Post-processing: Convert logits to probabilities
import torch.nn.functional as F
probs = F.softmax(logits, dim=1)

# Get the predicted label (the class with the highest probability)
predicted_class_idx = torch.argmax(probs, dim=1).item()

# Map index to label
labels = ["NEGATIVE", "POSITIVE"]
predicted_label = labels[predicted_class_idx]
confidence_score = probs[0][predicted_class_idx].item()

print(f"Predicted label: {predicted_label} with confidence {confidence_score:.2f}")

# Combining data from multiple sources
--------------------------------------

# One powerful application of Transformer models is their ability to combine and process data from multiple sources. This is especially useful when you need to:
   1. Search across multiple databases or repositories.
   2. Consolidate information from different formats (text, images, audio)
   3. Create a unified view of related information

# For example, you could build a system that:
    1.Searches for information across databases in multiple modalities like text and image.
    
    2.Combines results from different sources into a single coherent response. For example, from an audio file and text description.
    
    3.Presents the most relevant information from a database of documents and metadata.


