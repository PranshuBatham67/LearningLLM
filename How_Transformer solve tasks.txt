#There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models, the general idea is the same. Owing to its flexible architecture, most models are a variant of an encoder, a decoder, or an encoder-decoder structure.

IMP NOTE : There are some LLM models which build on Transformer architecture like either they are encoder, decoder or encoder-decoder. But some LLM are not build on transformer architecture. So, those models are build on transformer architecture there solving pattern for a given problem is same but those are not they have different solving pattern.
  

# Transformer-Based LLMs
------------------------

1. The Transformer architecture, introduced in the "Attention Is All You Need" paper, revolutionized natural language processing by using a self-attention mechanism. 

2. This mechanism allows the model to weigh the importance of different words in a sequence to understand context, regardless of their position.
 
Ex: For example, in the sentence "The cat sat on the mat and it was fluffy," the model can directly link "it" to "cat," even though they are far apart.

----> Transformer based LLMs are typically fall into three categories like "encoder" , "decoder" , "encoder-decoder" model.

----> While the specific task (e.g., translation vs. generation) changes the model's structure, the fundamental "solving pattern" for these models is rooted in the attention mechanism, parallel processing, and layer-by-layer data transformation. ---> What it means??


# Non-Transformer LLM Architectures
-----------------------------------
There are some models exist that are not based on the Transformer architecture. These models often utilize older or alternative methods, each with its own "solving pattern."

Ex:

1. Recurrent Neural Networks (RNNs) and their variants (LSTMs, GRUs): Before Transformers, RNNs were the dominant architecture for sequence data. They process data sequentially, one token at a time, using a "hidden state" to maintain memory of previous tokens. This makes them efficient for inference but difficult to parallelize during training. The "solving pattern" here is a sequential, step-by-step process.

     ----> Explanation : An RNN would read the book one word at a time, in a strictly sequential order. As it reads each word, it tries to remember the most important details from the previous words.

     ----> Problem : As it read the book, it continuously update(overwrite) it's hidden state(memory) due to that when it reach at the end of the book, it solve the actual context of the book.

2. Structured State Space Models (SSMs): A more recent development, models like Mamba are gaining attention as an alternative to Transformers. They are designed to handle long sequences efficiently. Their "solving pattern" is different, using a selective state-space approach that allows them to remember or forget information based on the input token, giving them the ability to manage long-range dependencies with linear complexity.
 
     ---> Explanation : An SSM, like Mamba, also processes the book sequentially, but it has a much more sophisticated way of handling its memory. Instead of a single, simple hidden state, it uses a state-space approach that can selectively remember or forget information.

     ---> Solving pattern : The Selective Note-Taker 📝
     This selective state-space approach allows SSMs to efficiently carry crucial information (like the existence of Moby-Dick) over very long distances without getting bogged down by less important details. This is why they are so good at handling long sequences.

# Transformer approach to solve problem
----------------------------------------

# A Transformer would approach the task completely differently. It wouldn't read the book word-by-word. Instead, it would be like a hyper-efficient librarian who takes the entire book and can instantly see every single word.

# Transformer have an self-attention machanism which act as an index card for every word in the book.

# So, when it read any word then it see the index card of that word and see other words which is relevant to that word and try to setup the relationship between them. This is a time taking process because it take quadratic time to process the book. Means, If your input sequence has a length of n, the number of attention calculations is proportional to n 
2
 .

----> Drawback  : This "see everything at once" approach is computationally expensive. As the book gets longer, the number of "cross-references" grows quadratically, meaning a book twice as long requires four times the computational effort. This is the primary limitation that models like SSMs are trying to solve.

NOTE : For short sentence this is not a problem but for long sentences it takes a long time.



# How language models work
---------------------------

#Language models work by being trained to predict the probability of a word given the context of surrounding words.

#This gives them a foundational understanding of language that can generalize to other tasks.

--->There are two main approaches for training a transformer model:
   
   1. Masked language modeling (MLM): Used by encoder models like BERT, this approach randomly masks some tokens in the input and trains the model to predict the original tokens based on the surrounding context. This allows the model to learn bidirectional context (looking at words both before and after the masked word).

   2. Causal language modeling (CLM): Used by decoder models like GPT, this approach predicts the next token based on all previous tokens in the sequence. The model can only use context from the left (previous tokens) to predict the next token.


# Types of language models
--------------------------

In the Transformers library, language models generally fall into three architectural categories:
 
    1.Encoder-only models (like BERT): These models use a bidirectional approach to understand context from both directions. They’re best suited for tasks that require deep understanding of text, such as classification, named entity recognition, and question answering.

    2.Decoder-only models (like GPT, Llama): These models process text from left to right and are particularly good at text generation tasks. They can complete sentences, write essays, or even generate code based on a prompt.

    3.Encoder-decoder models (like T5, BART): These models combine both approaches, using an encoder to understand the input and a decoder to generate output. They excel at sequence-to-sequence tasks like translation, summarization, and question answering.


NOTE : Understanding which part of the Transformer architecture (encoder, decoder, or both) is best suited for a particular NLP task is key to choosing the right model. Generally, tasks requiring bidirectional context use encoders, tasks generating text use decoders, and tasks converting one sequence to another use encoder-decoders.
8*

1. Test Generation 