HOW DO TRANSFORMERS WORK?
-------------------------

# we will take a look at the architecture of Transformer models and dive deeper into the concepts of attention, encoder-decoder architecture, and more.

#History of Transformer
-----------------------

1. The Transformer architecture was introduced in June 2017. The focus of the original research was on translation tasks.

     -----> A translation task is the process of converting a sequence of text into another language. The goal is to preserve the original meaning, context, and nuance as accurately as possible.
     -----> Ex: Here are some common types of translation tasks that models like the Transformer are designed to handle:

             1.Machine Translation (MT): This is the most classic example. It involves translating text from one natural language to another.
            
             2.Transliteration: This isn't about translating the meaning but converting a word from the script of one language to the script of another, based on phonetic similarity.

             3.Code-Switching Translation: This involves translating text that mixes two or more languages within the same sentence or conversation, a common occurrence in bilingual communities.

            4.Style Transfer (as a form of translation): While not a traditional translation task, it uses the same sequence-to-sequence concept. It involves "translating" text from one style to another within the same language.


NOTE :

1.GPT-like (also called auto-regressive Transformer models)

2.BERT-like (also called auto-encoding Transformer models)

3.T5-like (also called sequence-to-sequence Transformer models)


#Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!

Question: What is fine-tuning or transfer-learning?
Answer: All the transformer models like GTP-5, BERT have been trained as language model means it trained on massive data. So, these models are self-supervised it means user don't need to provide the label means it extract the label from the user's input and learn itself. But, these things are useless in specific task such as speech translation, language conversion because they developed a statistical understanding on data on which it trained. So, this general pre-trained model goes through a process called "transfer-learning" or "fine-tuning". During this, process user need to provide the data with accurate label and fine-tune a model on the desired task.



Question: What is causal language modelling?
Answer: The task of this model is to predict the next word after reading the n previous words. In this, output is totally depend on the n previous words.

Question: What is mask language modelling?
Answer: In this, task of this model is just to predict the masked(missing) words in the sentence.


#Transformers are big models
----------------------------
NOTE : Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the models’ sizes as well as the amount of data they are pretrained on.


# Carbon footprint of Transformers
----------------------------------


#Pretraining is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.

#This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus(a collection of written or spoken texts) of data, and training can take up to several weeks.


Question: Fine-Tuning is done after the model's pretraining is done. So why we not trained the model for our final use case from the scratch?

Ans: Since, pretrained model is already trained on a lots of data and it have strong statistical understanding on that data and when we do fine-tuning after the pretraining then that data have some similarities with the trained data so we not put much effort for getting the desired results. Besides, pretrained model is already trained on the massive data so we don't need very large corpus of data these things save our time and efforts.


#General Transformer architecture
---------------------------------

The model is primarily composed of two blocks :
1. Encoder(left): he encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.

2. Decoder(right): The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.

NOTE : Each of these parts can be used independently, depending on the task:
         1. Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.
 
         2. Decoder-only models: Good for generative tasks such as text generation.

         3. Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.



#Attention layers
-----------------

#A key feature of Transformer models is that they are built with special layers called attention layers.

#This layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.


# The original architecture of Transformers
-------------------------------------------

1.The Transformer architecture was originally designed for translation
2.During training, the encoder receives inputs (sentences) in a certain language, 
3.while the decoder receives the same sentences in the desired target language. 
4.In the encoder, the attention layers can use all the words in a sentence-->Why? Because we know, translation of a given can be dependent on what is after as well as before it in the sentence.
5.The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated).

Ex : For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.


#The attention mask can also be used in the encoder/decoder to prevent the model from paying attention to some special words — for instance, the special padding word used to make all the inputs the same length when batching together sentences.


# Architectures vs. checkpoints
-------------------------------

As we dive into Transformer models in this course, you’ll see mentions of architectures and checkpoints as well as models. These terms all have slightly different meanings:
      1.Architecture: This is the skeleton of the model — the definition of each layer and each operation that happens within the model.

      2.Checkpoints: These are the weights that will be loaded in a given architecture.

      3.Model: This is an umbrella term that isn’t as precise as “architecture” or “checkpoint”: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.

